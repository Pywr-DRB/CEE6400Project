#!/usr/bin/env python3
# scripts/compute_baseline_metrics.py
"""
Compute Pywr-DRB default-run baseline objective values per reservoir, using the
same ObjectiveCalculator and metric sets as optimization.

Inputs
------
- Default per-reservoir series (CSV) produced by build_default_timeseries.py:
    {DEFAULT_SERIES_DIR}/{res}_default_release.csv  (date,value)
    {DEFAULT_SERIES_DIR}/{res}_default_storage.csv  (date,value)
- Observations via get_observational_training_data(reservoir_name=..., data_dir=PROCESSED_DATA_DIR)
- Config-driven objective definitions (RELEASE_METRICS, STORAGE_METRICS), dates (VAL_START/VAL_END),
  reservoir capacities, inertia settings, etc.

Outputs
-------
- {FIG_DIR}/{BASELINE_DIR_NAME}_{BASELINE_INFLOW_TAG}/
    baseline_objectives_{res}_{VAL_START}_to_{VAL_END}.csv
    (columns: metric, pywr_baseline)

Notes
-----
- If a reservoir lacks observed releases, release metrics are skipped (left out).
- If default CSV missing for a reservoir, that reservoir is skipped with a warning.
"""

from __future__ import annotations
from pathlib import Path
from typing import Optional, Tuple, List
import argparse
import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parents[1]   # .../CEE6400Project
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import numpy as np
import pandas as pd
import os

# ---------------- project config & utilities ----------------
from methods.config import (
    FIG_DIR, PROCESSED_DATA_DIR,
    reservoir_options as RESERVOIR_NAMES,
    BASELINE_DIR_NAME, BASELINE_INFLOW_TAG,
    VAL_START, VAL_END,
    RELEASE_METRICS, STORAGE_METRICS,
    reservoir_capacity, INERTIA_BY_RESERVOIR, release_max_by_reservoir,
)

from methods.metrics.objectives import ObjectiveCalculator
from methods.load.observations import get_observational_training_data

OUTPUTS_DIR = Path(os.environ.get("DRB_OUTPUT_DIR", (ROOT / "pywr_data")))
DEFAULT_SERIES_DIR = Path(os.environ.get("DRB_DEFAULT_SERIES", (OUTPUTS_DIR / "_default_series")))

# --------------- helpers ----------------
def _ensure_dir(p: Path) -> Path:
    p.mkdir(parents=True, exist_ok=True)
    return p

def _read_default_series(res: str, defaults_dir: Path) -> Tuple[Optional[pd.Series], Optional[pd.Series]]:
    """Load default release/storage CSVs for a reservoir. Returns (release_MGD, storage_MG)."""
    rel_p = defaults_dir / f"{res}_default_release.csv"
    sto_p = defaults_dir / f"{res}_default_storage.csv"

    rel = None
    sto = None

    if rel_p.exists():
        try:
            df = pd.read_csv(rel_p)
            rel = pd.Series(df["value"].astype(float).values,
                            index=pd.to_datetime(df["date"]), name="pywr_release")
        except Exception as e:
            print(f"[WARN] Could not read {rel_p.name}: {e}")

    if sto_p.exists():
        try:
            df = pd.read_csv(sto_p)
            sto = pd.Series(df["value"].astype(float).values,
                            index=pd.to_datetime(df["date"]), name="pywr_storage")
        except Exception as e:
            print(f"[WARN] Could not read {sto_p.name}: {e}")

    return rel, sto

def _align(*series: pd.Series, start: str, end: str) -> List[pd.Series]:
    """Intersect indices across series, then slice [start, end]. Drops NAs."""
    idx = None
    for s in series:
        if s is None:
            continue
        idx = s.index if idx is None else idx.intersection(s.index)
    if idx is None or len(idx) == 0:
        return [pd.Series(dtype=float) for _ in series]
    idx = idx[(idx >= pd.to_datetime(start)) & (idx <= pd.to_datetime(end))]
    out = []
    for s in series:
        if s is None:
            out.append(pd.Series(dtype=float))
        else:
            out.append(s.loc[idx].astype(float).dropna())
    # re-intersect after dropna to enforce equal length
    idx2 = None
    for s in out:
        if len(s) == 0:
            continue
        idx2 = s.index if idx2 is None else idx2.intersection(s.index)
    if idx2 is None or len(idx2) == 0:
        return [pd.Series(dtype=float) for _ in series]
    return [s.loc[idx2] if len(s) else s for s in out]

def _calc_release_metrics(res: str, obs: pd.Series, sim: pd.Series) -> Optional[List[float]]:
    if obs is None or sim is None or len(obs) == 0 or len(sim) == 0:
        return None
    R_MAX = release_max_by_reservoir[res]
    iset  = INERTIA_BY_RESERVOIR[res]["release"]
    oc = ObjectiveCalculator(
        metrics=RELEASE_METRICS,
        inertia_tau=iset["tau"],
        inertia_scale_release=iset["scale"],
        inertia_release_scale_value=(R_MAX if iset["scale"] == "value" else None),
    )
    vals = oc.calculate(obs=obs.values.astype(np.float64),
                        sim=sim.values.astype(np.float64))
    return vals

def _calc_storage_metrics(res: str, obs: pd.Series, sim: pd.Series) -> Optional[List[float]]:
    if obs is None or sim is None or len(obs) == 0 or len(sim) == 0:
        return None
    iset  = INERTIA_BY_RESERVOIR[res]["storage"]
    oc = ObjectiveCalculator(
        metrics=STORAGE_METRICS,
        capacity_mg=reservoir_capacity[res],
        inertia_tau=iset["tau"],
        inertia_scale_storage=iset["scale"],
        inertia_storage_scale_value=iset["scale_value"],
    )
    vals = oc.calculate(obs=obs.values.astype(np.float64),
                        sim=sim.values.astype(np.float64))
    return vals

# --------------- main ----------------
def main():
    ap = argparse.ArgumentParser(description="Compute per-reservoir Pywr baseline objectives from default series & observations.")
    ap.add_argument("--defaults-dir", default=str(DEFAULT_SERIES_DIR), help="Dir with *_default_{release,storage}.csv")
    ap.add_argument("--outdir", default=None, help="Output dir for baseline CSVs (defaults to FIG_DIR/{BASELINE_DIR_NAME}_{BASELINE_INFLOW_TAG})")
    ap.add_argument("--start", default=str(VAL_START), help="Validation window start (YYYY-MM-DD)")
    ap.add_argument("--end",   default=str(VAL_END),   help="Validation window end (YYYY-MM-DD)")
    ap.add_argument("--reservoirs", nargs="*", default=list(RESERVOIR_NAMES), help="Subset of reservoirs to compute")
    args = ap.parse_args()

    defaults_dir = Path(args.defaults_dir).resolve()
    outdir = Path(args.outdir).resolve() if args.outdir else \
             _ensure_dir(Path(FIG_DIR) / f"{BASELINE_DIR_NAME}_{BASELINE_INFLOW_TAG}")
    _ensure_dir(outdir)

    print(f"[INFO] Defaults dir : {defaults_dir}")
    print(f"[INFO] Output dir   : {outdir}")
    print(f"[INFO] Window       : {args.start} to {args.end}")

    for res in args.reservoirs:
        print(f"\n=== Reservoir: {res} ===")

        # 1) default series
        rel_def, sto_def = _read_default_series(res, defaults_dir)
        if (rel_def is None or len(rel_def) == 0) and (sto_def is None or len(sto_def) == 0):
            print(f"[SKIP] No default series for {res} in {defaults_dir}")
            continue

        # 2) observations
        inflow_df, release_df, storage_df = get_observational_training_data(
            reservoir_name=res, data_dir=PROCESSED_DATA_DIR, as_numpy=False, inflow_type="inflow_pub"
        )
        rel_obs = None if release_df is None or release_df.empty else release_df.squeeze().astype(float).rename("obs_release")
        sto_obs = None if storage_df is None or storage_df.empty else storage_df.squeeze().astype(float).rename("obs_storage")

        # 3) align
        rel_obs_al, sto_obs_al, rel_def_al, sto_def_al = _align(
            rel_obs, sto_obs, rel_def, sto_def, start=args.start, end=args.end
        )

        # 4) compute metrics (skip a block if its inputs are missing)
        rows = []
        r_vals = _calc_release_metrics(res, rel_obs_al, rel_def_al)
        if r_vals is not None:
            for name, val in zip(RELEASE_METRICS, r_vals):
                rows.append((name, float(val)))

        s_vals = _calc_storage_metrics(res, sto_obs_al, sto_def_al)
        if s_vals is not None:
            for name, val in zip(STORAGE_METRICS, s_vals):
                rows.append((name, float(val)))

        if not rows:
            print(f"[SKIP] No comparable metrics could be computed for {res} (missing data).")
            continue

        out = pd.DataFrame(rows, columns=["metric", "pywr_baseline"])
        out_path = outdir / f"baseline_objectives_{res}_{args.start}_to_{args.end}.csv"
        out.to_csv(out_path, index=False)
        print(f"[OK] Wrote {out_path} with {len(out)} metrics")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[FATAL] {e}", file=sys.stderr)
        sys.exit(2)
